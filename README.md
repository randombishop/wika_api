# Wika Network API

This repo publishes a simple REST API to consume the data sources built with the [Wika ETL](https://github.com/randombishop/wika_etl)

Current version comes with 5 operations:
* `ping` => Healthcheck
* `/user/:user/liked_urls` => List the Urls liked by a user
* `/user/:user/owned_urls` => List the Urls owned by a user
* `/user/:user/search/:query` => Execute a search query against the elastic search Url index
* `/user/:user/recommend` => Generate recommendations for a specific user based on their Likes/Ownerships, network connections, and keywords

The repo was developed with [NestJS framework](https://docs.nestjs.com/).

It relies on 2 data sources: Neo4j and ElasticSearch, 
with the schemas defined by the Wika ETL repo, responsible for syncing between the blockchain and these databases.

If you have an instance of Wika ETL up and running, you can plug this API to these databases with a read only connection.

For development and testing, a little snapshot of the databases is provided in the `test_db` folder, 
along with a docker file to get them quickly up and running.


## Codebase overview

The project remains very close to the default starter code generated by NestJS framework in the `api` folder:
- `app.controller.ts` has all the operations along with their REST configuration and Swagger documentation decorators.
- The functions to connect to the databases and fetch data are built into `neo4j.service.ts` and `elastic.service.ts`
- The data types returned by the endpoints are defined in `src/types`
- Unit tests are provided for each source file in the `*.spec.ts` counterpart.

Most of the code is very straightforward data pulling queries, plus the `search` and `recommend` functions. 
These are MVP implementations to kick start the Wika Network, and should evolve once we gain more data. 

### Search
The search function relies on the Elastic Search `query_string` feature. 
For examples, formats and options supported by Elastic Search engine, refer to https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html

### Recommend
The recommendation engine generates recommendations for a specific user based on their 
Likes/Ownerships, network connections, and keywords.

It performs the following algorithm:
1. get connectedUrls using `neo4j.listUrlsByNetwork(user)`: for example, if the user U1 liked a website,
and the same website was liked or owned by a another user U2, all Urls connected to U2 are considered connected.
The function also orders all connected Urls by total number of likes and returns the top 100
2. The connected urls are passed to Elastic Search `More Like This` search method, returning a list of similar documents ordered by matching score.
3. Adds the total numbers of likes and the user number of likes to the search results.


## Using the repo

### Pre-requesites
- You will need:
  * `NodeJS` 
  * `yarn` 
  * `git`
  * `docker`
  * `docker-compose`
  

- Clone the repo.
```
git clone https://github.com/randombishop/wika_api
```
- Except for setting up the test database, you need to `cd` into the `api` folder to run all the repo operations.

### Starting up dev/test databases
Go to `test_db` folder

```cd test_db```

Unzip the provided `data.tar.gz`

```
tar -xf data.tar.gz
```

Start with `docker-compose`

```
docker-compose up
```

You can connect to the Neo4j instance with `http://localhost:7474/browser/`

And for the Elastic Search `http://localhost:5601`

You can find the user names and passwords in the `docker-compose.yml` file you just started.

Leave the databases up and running in a separate terminal and start a new one to run the actual API. 

CD into the `api` folder for the rest of this guide.


### Configure env variables to connect to your databases.
The API uses the following environment variables to configure access to Neo4J and Elastic Search
```
NEO4J_HOST="bolt://localhost"
NEO4J_USER="neo4j"
NEO4J_PASS="1234"
ES_HOST="http://localhost:9200"
ES_USER="elastic"
ES_PASS="abcd"
```

It has the `dotenv` module enabled, so you can pass them either through the OS environment or using a `.env` file.

A file `.env-example` is provided with the same credentials as in the test databases, 
you can simply copy it into `.env` to get started.

(`.env` file is part of `.gitignore` to exclude users' credentials from the github repo.)

```
cp .env-example .env .
```


### Install NodeJS dependencies
```
yarn install
```

### Compile Typescript code
```
yarn build
```

### Format with prettier
```
yarn format
```

### Lint
```
yarn lint
```

### Running the tests
```
yarn test
```

### Start the API
To start normally
```
yarn start
```

To start and update on code changes
```
yarn start:dev
```

Once the API is up and running, navigate to `http://localhost:3000/doc` for the online documentation of the API 
and direct tests (Swagger)


### Optional: using docker
The repo runs easily with simple `NodeJS` and `yarn` pre-requisites, but we also provide a `Dockerfile` to start it 
as a container.

Simply configure database access in `docker-compose` and `docker-compose up` to use it this way.

The `.env` file is excluded from the docker image to avoid interference, the preferred way to configure 
the container is through the `docker-compose.yaml` file. The defaults there should work against the test databases.





